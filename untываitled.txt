Привет подскажи мне по моему проекту

Вот его код

Вот так выглядит корневая папка

pwd
/Users/deal/Desktop/news_analysis
❯ ls
LICENSE bot inference scrappers temp_news.txt
README.md eda model temp_file.txt

Переходим в папку bot

ls
pycache handlers main.py requirements.txt
config_reader.py keyboards news_file.txt temp_file.txt

Вот файл main.py

import logging
import asyncio
from aiogram import Router, F
from aiogram import Bot, Dispatcher, types
from aiogram.enums import ParseMode
from aiogram.types import ReplyKeyboardMarkup, KeyboardButton
from aiogram.filters import Command, StateFilter
from aiogram.utils.keyboard import InlineKeyboardBuilder
from config_reader import config
from handlers import about_work, common, classification_news

Устанавливаем уровень логгирования
logging.basicConfig(level=logging.INFO)

bot = Bot(token=config.bot_token.get_secret_value())
dp = Dispatcher()

async def main():
dp.include_routers(common.router, about_work.router, classification_news.router)
await dp.start_polling(bot)

if name == 'main':
asyncio.run(main())

Вот файл config_reader.py

from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import SecretStr

class Settings(BaseSettings):

Желательно вместо str использовать SecretStr
для конфиденциальных данных, например, токена бота
bot_token: SecretStr

bash
Копировать код

Начиная со второй версии pydantic, настройки класса настроек задаются
через model_config
В данном случае будет использоваться файла .env, который будет прочитан
с кодировкой UTF-8
model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8')
При импорте файла сразу создастся
и провалидируется объект конфига,
который можно далее импортировать из разных мест
config = Settings()

Вот содержание папки handlers

ls
pycache about_work.py classification_news.py common.py

Вот файл about_work.py

from aiogram import Router, F
from aiogram import types

from keyboards.simple_row import make_row_keyboard, make_inline_keyboard
router = Router()

@router.callback_query(F.data == "Рассказать о моей работе")
async def callback_check(callback: types.CallbackQuery):
await callback.message.answer(
text="Я в отпуске хахах:)"
)

Вот файл classification_news.py

from aiogram import F, Router, types
from aiogram.types import Message
import subprocess
import logging
import os

router = Router()

url = 'http://127.0.0.1:8000/uploadnews/'

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(file)))
temp_file_path = os.path.join(BASE_DIR, 'temp_file.txt')

@router.callback_query(F.data == "Классифицировать новость")
async def callback_check(callback: types.CallbackQuery):
await callback.message.answer(
text="Пришлите текст новости:)"
)

@router.message(F.text)
async def text_classification(message: Message):
text = message.text

python
Копировать код

Логируем текст новости
logging.info(f"Received news text: {text[:100]}...") # Логируем первые 100 символов текста

with open(temp_file_path, 'w', encoding='utf-8') as temp_file:
temp_file.write(text)

try:

lua
Копировать код
result = subprocess.run(
    ["curl", "-X", "POST", url, "-F", f"news=@{temp_file_path}"],
    capture_output=True,
    text=True
)


if result.returncode != 0:
    raise subprocess.CalledProcessError(result.returncode, result.args, output=result.stdout, stderr=result.stderr)


logging.info(f"Server response: {result.stdout}")


content_class = result.stdout.strip()
except subprocess.CalledProcessError as e:
logging.error(f"Curl command error: {e.stderr}")
content_class = 'Ошибка при классификации'
except Exception as err:
logging.error(f"Other error occurred: {err}")
content_class = 'Ошибка при классификации'

await message.answer(f"Тема этой новости: {content_class}")
Вот файл common.py

from aiogram import F, Router
from aiogram.filters import Command
from aiogram.filters import StateFilter
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import default_state
from aiogram.types import Message, ReplyKeyboardRemove
from keyboards.simple_row import make_row_keyboard, make_inline_keyboard

router = Router()

Клавиатура с основными кнопками
kb = make_inline_keyboard(["Рассказать о моей работе", "Классифицировать новость"])

Хэндлер на команду /start
@router.message(Command('start'))
async def handle_start(message: Message):
await message.answer(
"Привет! Я бот для классификации новостей.\n"
"Я могу помочь двумя способами - \n"
"Первый - рассказать о том как я работаю. \n"
"Второй - классифицировать новость по тексту. \n"
"Выбери одну из кнопок в меню(если запутались используйте /cancel):",
reply_markup=kb
)

Нетрудно догадаться, что следующие два хэндлера можно
спокойно объединить в один, но для полноты картины оставим так
default_state - это то же самое, что и StateFilter(None)
@router.message(StateFilter(None), Command(commands=["cancel"]))
@router.message(default_state, F.text.lower() == "отмена")
async def cmd_cancel_no_state(message: Message, state: FSMContext):

Стейт сбрасывать не нужно, удалим только данные
await state.set_data({})
await message.answer(
text="Для начала нажмите /start",
reply_markup=ReplyKeyboardRemove()
)

@router.message(Command(commands=["cancel"]))
@router.message(F.text.lower() == "отмена")
async def cmd_cancel(message: Message, state: FSMContext):
await state.clear()
await message.answer(
text="Действие отменено",
reply_markup=ReplyKeyboardRemove()
)

Вот содержание папки keyboards

ls
pycache simple_row.py

Вот файл simple_rw.py

from aiogram.types import ReplyKeyboardMarkup, KeyboardButton, InlineKeyboardButton, InlineKeyboardMarkup

def make_row_keyboard(items: list[str]) -> ReplyKeyboardMarkup:
"""
Создаёт реплай-клавиатуру с кнопками в один ряд
items: список текстов для кнопок
:return: объект реплай-клавиатуры
"""
row = [KeyboardButton(text=item) for item in items]
return ReplyKeyboardMarkup(keyboard=[row], resize_keyboard=True)

def make_inline_keyboard(items: list[str]) -> InlineKeyboardMarkup:
"""
Создаёт инлайн-клавиатуру с кнопками в один ряд
items: список текстов для кнопок
:return: объект инлайн-клавиатуры
"""
row = [InlineKeyboardButton(text=item, callback_data=item) for item in items]
return InlineKeyboardMarkup(inline_keyboard=[row])

Вот содержание папки inference

ls
pycache model20000.cbm tfidf_vectorizer20000.pkl
app.py roberta_model.py tokenizer_config.json
config.json special_tokens_map.json vocab.json
merges.txt temp_file.txt
model.py temp_news.txt
❯ pwd
/Users/deal/Desktop/news_analysis/inference

Вот файл app.py

from fastapi import FastAPI, UploadFile, HTTPException
from roberta_model import predict_by_filename

import logging

app = FastAPI()

logging.basicConfig(level=logging.INFO)

@app.post('/uploadnews/')
async def classificate_news(news: UploadFile):
try:
with open(news.filename, 'wb') as f:
content = await news.read()
f.write(content)

python
Копировать код
logging.info(f"Received file: {news.filename}")

python
Копировать код
content_class = str(predict_by_filename(news.filename))
logging.info(f"Classified news as: {content_class}")

return content_class
except Exception as e:
logging.error(f"Error processing news: {e}")
raise HTTPException(status_code=500, detail="Ошибка при классификации")
Вот файл model.py

import joblib
from catboost import CatBoostClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger
from nltk.corpus import stopwords

vectorizer = joblib.load('tfidf_vectorizer20000.pkl')
model = CatBoostClassifier()
model.load_model('model20000.cbm')
classes_names = ['Культура', 'Россия', 'Мир', 'Наука и технологии', 'Спорт', 'Экономика', 'Путешествия']

stop_words = stopwords.words('russian')
stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'за', 'из', 'из-за', 'на', 'ок', 'кстати', 'который', 'мочь', 'весь', 'еще', 'также', 'свой', 'ещё', 'самый', 'ул', 'комментарий', 'английский', 'язык'])

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)

def text_prep(text) -> str:
doc = Doc(text)
doc.segment(segmenter)
doc.tag_morph(morph_tagger)

arduino
Копировать код
for token in doc.tokens:
token.lemmatize(morph_vocab)

lemmas = [_.lemma for _ in doc.tokens]
words = [lemma for lemma in lemmas if lemma.isalpha() and len(lemma) > 2]
filtered_words = [word for word in words if word not in stop_words]
return " ".join(filtered_words)
def predict_by_text(text):
news_text = text_prep(text)
embedding = vectorizer.transform([news_text])
number_of_class = model.predict(embedding)[0]
return classes_names[number_of_class]

Вот файл roberta model.py именно этот файл являетяс файлом нашей модели

import os
import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger
from nltk.corpus import stopwords
import logging

model_path = os.path.abspath(os.path.join(os.path.dirname(file), '../model'))
logging.info(f"Model path: {model_path}")
logging.info(f"Contents of model directory: {os.listdir(model_path)}")

try:
tokenizer = RobertaTokenizer.from_pretrained(model_path)
model = RobertaForSequenceClassification.from_pretrained(model_path)
except Exception as e:
logging.error(f"Error loading model: {e}")
raise

class_names = ['Культура', 'Россия', 'Мир', 'Наука и технологии', 'Спорт', 'Экономика', 'Путешествия']

stop_words = stopwords.words('russian')
stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'за', 'из', 'из-за', 'на', 'ок', 'кстати',
'который', 'мочь', 'весь', 'еще', 'также', 'свой', 'ещё', 'самый', 'ул', 'комментарий',
'английский', 'язык'])

segmenter = Segmenter()
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)

def text_prep(text) -> str:
doc = Doc(text)
doc.segment(segmenter)
doc.tag_morph(morph_tagger)

arduino
Копировать код
for token in doc.tokens:
token.lemmatize(morph_vocab)

lemmas = [_.lemma for _ in doc.tokens]
words = [lemma for lemma in lemmas if lemma.isalpha() and len(lemma) > 2]
filtered_words = [word for word in words if word not in stop_words]
return " ".join(filtered_words)
def news_embedding(news_text):
return tokenizer(news_text, padding=True, truncation=True, return_tensors="pt")

def predict_by_filename(filename):
try:
with open(filename, 'r') as f:
news_text = f.read()

scss
Копировать код
logging.info(f"Processing text: {news_text[:100]}...") # Логируем первые 100 символов текста

scss
Копировать код
processed_text = text_prep(news_text)
inputs = news_embedding(processed_text)
outputs = model(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
predicted_class = torch.argmax(predictions, dim=1).item()

return class_names[predicted_class]
except Exception as e:
logging.error(f"Error in prediction: {e}")
raise
Вот содержание папки model

pwd
/Users/deal/Desktop/news_analysis/model
❯ ls
best_model.pkl special_tokens_map.json
catboost_classifier.pkl tfidf_vectorizer20000.pkl
config.json tokenizer_config.json
merges.txt vocab.json
model20000.cbm Обучение модели.ipynb
pytorch_model.bin

Если я запущу его и пользователи сделают несколько запросов параллельно бот будет нормально рабоать?



Вот поправки которые я внес в файлы app.py и classification_news.py для большей асинхронности

import asyncio
import os
import tempfile
import subprocess
from aiogram import F, Router, types
from aiogram.types import Message
import logging

router = Router()

url = 'http://127.0.0.1:8000/uploadnews/'

@router.callback_query(F.data == "Классифицировать новость")
async def callback_check(callback: types.CallbackQuery):
await callback.message.answer(
text="Пришлите текст новости:)"
)

@router.message(F.text)
async def text_classification(message: Message):
text = message.text

python
Копировать код
logging.info(f"Received news text: {text[:100]}...")

try:
    # Создаем уникальный временный файл во временной папке
    temp_dir = tempfile.mkdtemp()
    temp_file_path = os.path.join(temp_dir, 'temp_file.txt')

    # Записываем текст в временный файл
    with open(temp_file_path, 'w', encoding='utf-8') as temp_file:
        temp_file.write(text)

    # Выполняем HTTP запрос с использованием curl
    result = subprocess.run(
        ["curl", "-X", "POST", url, "-F", f"news=@{temp_file_path}"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        raise subprocess.CalledProcessError(result.returncode, result.args, output=result.stdout, stderr=result.stderr)

    content_class = result.stdout.strip()
except subprocess.CalledProcessError as e:
    logging.error(f"Curl command error: {e.stderr}")
    content_class = 'Ошибка при классификации'
except Exception as err:
    logging.error(f"Other error occurred: {err}")
    content_class = 'Ошибка при классификации'

await message.answer(f"Тема этой новости: {content_class}")
from fastapi import FastAPI, UploadFile, HTTPException
import tempfile
import os
import logging

app = FastAPI()

logging.basicConfig(level=logging.INFO)

@app.post('/uploadnews/')
async def classificate_news(news: UploadFile):
try:
# Создаем уникальный временный файл во временной папке
temp_dir = tempfile.mkdtemp()
temp_file_path = os.path.join(temp_dir, 'temp_file.txt')

python
Копировать код
    # Записываем данные из файла в временный файл
    with open(temp_file_path, 'wb') as f:
        content = await news.read()
        f.write(content)

    logging.info(f"Received file: {news.filename}")

    # Выполняем классификацию новости и возвращаем результат
    content_class = str(predict_by_filename(temp_file_path))
    logging.info(f"Classified news as: {content_class}")

    return content_class
except Exception as e:
    logging.error(f"Error processing news: {e}")
    raise HTTPException(status_code=500, detail="Ошибка при классификации")
Теперь бот будет асинхронным?
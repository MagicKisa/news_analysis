{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35081330",
   "metadata": {},
   "source": [
    "## Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c33e6f",
   "metadata": {},
   "source": [
    "### Загружаем необходимые библиотеки и модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "sns.set(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be918dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
    "\n",
    "# import mlxtend\n",
    "# from mlxtend.evaluate import paired_ttest_kfold_cv\n",
    "\n",
    "# from plotly.offline import iplot\n",
    "# import cufflinks as cf\n",
    "# cf.go_offline()\n",
    "# cf.set_config_file(offline=False, world_readable=True)\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c226473",
   "metadata": {},
   "source": [
    "### Загружаем данные и разделяем выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4847d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_lenta_interfax.csv', parse_dates=['date']).drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c6106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_title'] = df['title'].str.len()\n",
    "df['len_content'] = df['content'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36eca1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Russia                169748\n",
       "world                 129589\n",
       "ekonomika              77595\n",
       "sport                  35053\n",
       "science_technology     29867\n",
       "kultura                24028\n",
       "traveling              22255\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Кол-во и распределение тематик\n",
    "df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516051c",
   "metadata": {},
   "source": [
    "Отберем по 20000 примеров для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89ca8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_topics = df['topic'].unique()\n",
    "df_1 = pd.DataFrame()\n",
    "# Создаем выборку для каждой тематики\n",
    "for topic in unique_topics:\n",
    "    topic_data = df[df['topic'] == topic].sample(frac=1, random_state=random_state).head(20000) # Выбираем по 20000 примеров для каждой тематики\n",
    "    df_1 = pd.concat([df_1, topic_data])  # Объединяем выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eefe36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kultura               20000\n",
       "Russia                20000\n",
       "world                 20000\n",
       "science_technology    20000\n",
       "sport                 20000\n",
       "ekonomika             20000\n",
       "traveling             20000\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054c206",
   "metadata": {},
   "source": [
    "## Препроцессинг текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97371a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "stop_words.extend(['что', 'это', 'так',\n",
    "                    'вот', 'быть', 'как',\n",
    "                    'в', '—', 'к', 'за', 'из', 'из-за',\n",
    "                    'на', 'ок', 'кстати',\n",
    "                    'который', 'мочь', 'весь',\n",
    "                    'еще', 'также', 'свой',\n",
    "                    'ещё', 'самый', 'ул', 'комментарий',\n",
    "                    'английский', 'язык'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c40cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(text) -> str:\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "\n",
    "    lemmas = [_.lemma for _ in doc.tokens]\n",
    "    words = [lemma for lemma in lemmas if lemma.isalpha() and len(lemma) > 2]\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fc10ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4h 56min 45s\n",
      "Wall time: 7h 10min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['title_clean'] = df.title.apply(text_prep)\n",
    "df['content_clean'] = df.content.apply(text_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f960b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Bek Air возобновит работу после авиакатастрофы в Казахстане не ранее 10 января',\n",
       "        'bek air возобновить работа авиакатастрофа казахстан ранее январь']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(1)[['title', 'title_clean']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81899a4",
   "metadata": {},
   "source": [
    "## Строим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ada55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.content_clean.str.split(),\n",
    "                                                    df.topic.values,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=random_state)\n",
    "model = Word2Vec(sentences=X_train,\n",
    "                 vector_size=200,\n",
    "                 min_count=10,\n",
    "                 window=2,\n",
    "                 seed=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a5434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    \"\"\"Get tfidf weighted vectors\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.word2vec = model.wv\n",
    "        self.word2weight = None\n",
    "        self.dim = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of\n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec.get_vector(w) * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3dcb7",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160317be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87     50876\n",
      "           1       0.84      0.87      0.86     23453\n",
      "           2       0.92      0.89      0.90      7066\n",
      "           3       0.85      0.86      0.86      8922\n",
      "           4       0.98      0.98      0.98     10528\n",
      "           5       0.86      0.84      0.85      6701\n",
      "           6       0.87      0.86      0.86     38895\n",
      "\n",
      "    accuracy                           0.88    146441\n",
      "   macro avg       0.89      0.88      0.88    146441\n",
      "weighted avg       0.88      0.88      0.88    146441\n",
      "\n",
      "CPU times: total: 16h 15min 50s\n",
      "Wall time: 1d 6h 50min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "tfidf_vectorizer = TfidfEmbeddingVectorizer(model)\n",
    "tfidf_vectorizer.fit(X_train, y_train)\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "params = {'iterations': 100, \n",
    "          'learning_rate': 0.13, \n",
    "          'depth': 7, \n",
    "          'verbose': False}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('w2v', TfidfEmbeddingVectorizer(model)),\n",
    "    ('clf', CatBoostClassifier(**params, random_state=random_state, task_type=\"GPU\", devices='0'))\n",
    "    ])\n",
    "\n",
    "# Определение сетки значений для GridSearchCV\n",
    "grid = {\n",
    "    'clf__iterations': [100, 200, 300],\n",
    "    'clf__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'clf__depth': [5, 7, 9],\n",
    "}\n",
    "\n",
    "# Инициализация GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, grid, cv=5, n_jobs=None)\n",
    "\n",
    "# Обучение модели с использованием GridSearchCV\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Оценка качества модели\n",
    "print(classification_report(y_test_encoded, grid_search.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e722cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__depth': 9, 'clf__iterations': 300, 'clf__learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ddde93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catboost_classifier.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Сохранение лучших параметров и весов модели\n",
    "joblib.dump(grid_search.best_estimator_.named_steps['clf'], 'catboost_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977c703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
